{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの学習\n",
    "参考図書：ゼロから作るDeep Learning  \n",
    "このノートでは、参考書の第４章以降、すなわちニューラルネットワークの学習、誤差逆伝播などの実装技術を学ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "### 二乗和誤差\n",
    "一般には次の二乗和誤差を用いることが多い。\n",
    "\n",
    "$$ E = \\frac{1}{2}\\sum_{k}(y_k - t_k)^2 $$\n",
    "\n",
    "$y_k$はニューラルネットワークの出力、$t_k$は教師データを表し、$k$はデータの次元数を表す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# [2]を正解とする教師データ\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 例１：[2]の確率が最も高い場合（0.6）\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "# 例２：[7]の確率が最も高い場合（0.6）\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交差エントロピー誤差\n",
    "$$ E = - \\sum_{k}t_k\\log{y_k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y + delta)) # 微小量deltaを加えることでマイナスの無限大-infの出力を防ぐ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例えば\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 正解ラベルは3番目\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ学習\n",
    "今までは一つのデータに対して損失関数を考えていたが、実際には訓練データの数が100個あれば、その100個の損失関数の和を指標とする。  \n",
    "すなわち\n",
    "$$ E = - \\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log{y_{nk}} $$\n",
    "\n",
    "しかし、巨大なデータを扱うには、一気に損失関数の総和を求めるには時間がかかりすぎる。  \n",
    "そこで、訓練データからある枚数だけを選び出し（これをミニバッチという）、それを全体の損失関数の近似とみなして、ミニバッチごとに学習を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10, 784)\n",
      "(10, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([56230, 19727,  4741, 40925, 51444, 11163,  3773, 11199, 53359,\n",
       "       31692])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNISTデータからミニバッチを選び出すコード\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "# 訓練データの中からランダムに10枚だけ抜き出す\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # train_size未満の自然数からランダムに10個の数字を選び出す\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(x_batch.shape)\n",
    "print(t_batch.shape)\n",
    "\n",
    "# np.random.choiceの補足\n",
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差をミニバッチ学習に対応させる\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # xと同じ形状の配列を生成\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h)の計算\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h)の計算\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 値をもとに戻す\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 勾配計算の実験\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # または return np.sum(x**2)\n",
    "    \n",
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配降下法の実装\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引数のfは最適化したい関数、init_xは初期値、lrはlearning rateを意味する学習率、step_numは勾配法による繰り返しの数  \n",
    "試しに次の問題に適用してみる。\n",
    "\n",
    "$ f(x_0, x_1) = {x_0}^2 + {x_1}^2 $の最小値を勾配法で求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はほぼ0だから、正しい結果。  \n",
    "学習率や繰り返しの数は人間が適切に設定しなければならない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単なニューラルネットワークを例に勾配を求める実装\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    # 形状2×3の重みパラメータ\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) # ガウス分布で初期化\n",
    "        \n",
    "    # 予測のためのメソッド\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    # 損失関数の値を求めるメソッド\n",
    "    # 引数のxには入力データ、tには正解ラベルが入力される\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95359659  0.11174246  0.32618324]\n",
      " [ 0.05947028  0.40103842  0.70174092]]\n",
      "[-0.5186347   0.42798005  0.82727677]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6580864787950823"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 実験\n",
    "net = simpleNet()\n",
    "print(net.W) # 重みパラメータ\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p)) # 最大値のインデックス\n",
    "\n",
    "t = np.array([0, 0, 1]) # 正解ラベル\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08087715  0.20841808 -0.28929523]\n",
      " [ 0.12131572  0.31262712 -0.43394285]]\n"
     ]
    }
   ],
   "source": [
    "# 勾配を求める\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習手順のまとめ\n",
    "* 前提\n",
    "    * 適応可能な重みとバイアスがあり、これらを訓練データに適応するように調整することを学習と呼ぶ\n",
    "* ステップ1（ミニバッチ）\n",
    "    * 訓練データの中からランダムに一部のデータを選び出す\n",
    "    * これをミニバッチと呼ぶ\n",
    "    * ミニバッチの損失関数の値を減らすことをまずは目的に\n",
    "* ステップ2（勾配の算出）\n",
    "    * ミニバッチの損失関数を減らすために、各重みのパラメータの勾配を求める\n",
    "    * 勾配＝損失関数の値をもっと減らす方向（ベクトル）\n",
    "* ステップ3（パラメータの更新）\n",
    "    * 重みパラメータを勾配方向に微小量（＝学習率 learning rate）だけ更新する\n",
    "* ステップ4（繰り返し）\n",
    "    * ステップ1~3を繰り返す（＝反復回数iterationを指定）\n",
    "    \n",
    "この手法は、確率的勾配降下法（＝stochastic gradient descent or **SGD**)と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2層ニューラルネットワークのクラスの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {} # ニューラルネットワークのパラメータを保持するディクショナリ変数（インスタンス変数）\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) # 1層目の重み\n",
    "        self.params['b1'] = np.zeros(hidden_size) # 1層目のバイアス\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # 損失関数の値を求める\n",
    "    # x:入力データ、t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 認識精度を求める関数\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 重みパラメータに対する勾配を求める関数\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗算レイヤの実装\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # ここで順伝播時の入力値を再利用する\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# りんごの買い物の例\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加算レイヤの実装\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass # 初期化が必要ないので、何も行わない\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "# リンゴ2個とミカン3個の買い物の実装\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidレイヤの実装\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affineレイヤの実装\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax-with-Lossレイヤの実装\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11093333333333333 0.1085\n",
      "0.9042333333333333 0.91\n",
      "0.92125 0.923\n",
      "0.9357 0.9332\n",
      "0.9462333333333334 0.943\n",
      "0.9514166666666667 0.9492\n",
      "0.955 0.9524\n",
      "0.9589333333333333 0.9565\n",
      "0.9630666666666666 0.9596\n",
      "0.96555 0.9593\n",
      "0.9677166666666667 0.9626\n",
      "0.9709666666666666 0.9641\n",
      "0.9720333333333333 0.9656\n",
      "0.9736833333333333 0.9655\n",
      "0.9759 0.9659\n",
      "0.9759666666666666 0.9691\n",
      "0.9763 0.9661\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
